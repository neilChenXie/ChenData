a1.sources = r1
a1.sinks = k1
a1.channels = c1 c2

# Describe/configure the source
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /home/hadoop/tmp
#a1.sources.r1.fileHeader = true
a1.sources.r1.interceptors = i0 i1
a1.sources.r1.interceptors.i0.type = regex_filter
a1.sources.r1.interceptors.i0.regex = ^((\\d+\\.){3}\\d+).*"GET\ \/news\/detail\\?.*newId=(\\d+)
a1.sources.r1.interceptors.i1.type = regex_extractor
a1.sources.r1.interceptors.i1.regex = ^((\\d+\\.){3}\\d+).*"GET\ \/news\/detail\\?.*newId=(\\d+)
#a1.sources.r1.interceptors.i1.regex = ^(\\d+\\.{3}\\d)\ *\"GET\ /news/detail\\?
a1.sources.r1.interceptors.i1.serializers = s1 s2 s3
a1.sources.r1.interceptors.i1.serializers.s1.name = one
a1.sources.r1.interceptors.i1.serializers.s2.name = two
a1.sources.r1.interceptors.i1.serializers.s3.name = three
#a1.sources.r1.selector.type = multiplexing
#a1.sources.r1.selector.header = multiplexing
#a1.sources.r1.selector.mapping. = c1
#a1.sources.r1.selector.mapping. = c2
#a1.sources.r1.selector.default = c2

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
a1.channels.c2.type = memory
a1.channels.c2.capacity = 1000
a1.channels.c2.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1 c2
a1.sinks.k1.channel = c1
